{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "33CFUAwObO88"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import warnings\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data exploration and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jiBusib9xUgu"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Newsgroups: alt.atheism\\nPath: cantaloupe.srv....</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Data       Labels\n",
       "0  Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...  alt.atheism\n",
       "1  Newsgroups: alt.atheism\\nPath: cantaloupe.srv....  alt.atheism\n",
       "2  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...  alt.atheism\n",
       "3  Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...  alt.atheism\n",
       "4  Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53...  alt.atheism"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog = pd.read_csv(\"C:\\\\Users\\\\hp\\\\Downloads\\\\blogs.csv\",encoding = \"ISO-8859-1\")\n",
    "blog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1668663033825,
     "user": {
      "displayName": "Salem Sameer Shaikh",
      "userId": "05291575250389740253"
     },
     "user_tz": -330
    },
    "id": "cwjrIiv_bO89",
    "outputId": "13a217f2-26c9-45c9-fbf3-cb9a94569028"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Data    2000 non-null   object\n",
      " 1   Labels  2000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 31.4+ KB\n"
     ]
    }
   ],
   "source": [
    "blog.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DOLDRUQbO89"
   },
   "source": [
    "#### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_Data(Data, remove_stopwords=True, use_stemming=False, use_lemmatization=False):\n",
    "    \n",
    "    # Initialize stemmer and lemmatizer\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Lowercase the text\n",
    "    Data = Data.lower()\n",
    "    # Remove punctuation\n",
    "    Data = re.sub(r'[^\\w\\s]', '', Data)\n",
    "    # Remove numbers\n",
    "    Data = re.sub(r'\\d+', '', Data)\n",
    "    # Tokenize the text\n",
    "    tokens = Data.split()\n",
    "    if remove_stopwords:\n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    if use_stemming:\n",
    "        # Apply stemming\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    if use_lemmatization:\n",
    "        # Apply lemmatization\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Join tokens back into a single string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "clean = lambda x: clean_Data(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1128,
     "status": "ok",
     "timestamp": 1668663034949,
     "user": {
      "displayName": "Salem Sameer Shaikh",
      "userId": "05291575250389740253"
     },
     "user_tz": -330
    },
    "id": "qYyw1K8gbO89",
    "outputId": "17a08dc4-21c2-4a6f-e7d5-cb4573d72331"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Labels</th>\n",
       "      <th>cleaned_Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>path cantaloupesrvcscmuedumagnesiumclubcccmued...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Newsgroups: alt.atheism\\nPath: cantaloupe.srv....</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>newsgroups altatheism path cantaloupesrvcscmue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>path cantaloupesrvcscmuedudasnewsharvardedunoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>path cantaloupesrvcscmuedumagnesiumclubcccmued...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>xref cantaloupesrvcscmuedu altatheism talkreli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu talk.abortion:...</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>xref cantaloupesrvcscmuedu talkabortion altath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu talk.religion....</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>xref cantaloupesrvcscmuedu talkreligionmisc ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu talk.origins:4...</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>xref cantaloupesrvcscmuedu talkorigins talkrel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu talk.religion....</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>xref cantaloupesrvcscmuedu talkreligionmisc al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.skeptic:43...</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>xref cantaloupesrvcscmuedu sciskeptic talkpoli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Data              Labels  \\\n",
       "0     Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...         alt.atheism   \n",
       "1     Newsgroups: alt.atheism\\nPath: cantaloupe.srv....         alt.atheism   \n",
       "2     Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...         alt.atheism   \n",
       "3     Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...         alt.atheism   \n",
       "4     Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53...         alt.atheism   \n",
       "...                                                 ...                 ...   \n",
       "1995  Xref: cantaloupe.srv.cs.cmu.edu talk.abortion:...  talk.religion.misc   \n",
       "1996  Xref: cantaloupe.srv.cs.cmu.edu talk.religion....  talk.religion.misc   \n",
       "1997  Xref: cantaloupe.srv.cs.cmu.edu talk.origins:4...  talk.religion.misc   \n",
       "1998  Xref: cantaloupe.srv.cs.cmu.edu talk.religion....  talk.religion.misc   \n",
       "1999  Xref: cantaloupe.srv.cs.cmu.edu sci.skeptic:43...  talk.religion.misc   \n",
       "\n",
       "                                           cleaned_Data  \n",
       "0     path cantaloupesrvcscmuedumagnesiumclubcccmued...  \n",
       "1     newsgroups altatheism path cantaloupesrvcscmue...  \n",
       "2     path cantaloupesrvcscmuedudasnewsharvardedunoc...  \n",
       "3     path cantaloupesrvcscmuedumagnesiumclubcccmued...  \n",
       "4     xref cantaloupesrvcscmuedu altatheism talkreli...  \n",
       "...                                                 ...  \n",
       "1995  xref cantaloupesrvcscmuedu talkabortion altath...  \n",
       "1996  xref cantaloupesrvcscmuedu talkreligionmisc ta...  \n",
       "1997  xref cantaloupesrvcscmuedu talkorigins talkrel...  \n",
       "1998  xref cantaloupesrvcscmuedu talkreligionmisc al...  \n",
       "1999  xref cantaloupesrvcscmuedu sciskeptic talkpoli...  \n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog['cleaned_Data'] = blog.Data.apply(clean)\n",
    "blog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert text data into TFIdf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "vectorizer = TfidfVectorizer(max_df= 1000, max_features= 10000)\n",
    "x = vectorizer.fit_transform(blog['cleaned_Data'])\n",
    "y = blog['Labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 10000) (400, 10000) (1600,) (400,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for train:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.93      0.99      0.96        82\n",
      "           comp.graphics       1.00      0.99      0.99        82\n",
      " comp.os.ms-windows.misc       0.99      0.99      0.99        78\n",
      "comp.sys.ibm.pc.hardware       0.97      0.99      0.98        75\n",
      "   comp.sys.mac.hardware       1.00      1.00      1.00        79\n",
      "          comp.windows.x       1.00      1.00      1.00        75\n",
      "            misc.forsale       0.99      1.00      0.99        82\n",
      "               rec.autos       1.00      1.00      1.00        82\n",
      "         rec.motorcycles       1.00      0.99      0.99        84\n",
      "      rec.sport.baseball       1.00      1.00      1.00        82\n",
      "        rec.sport.hockey       1.00      1.00      1.00        85\n",
      "               sci.crypt       1.00      1.00      1.00        81\n",
      "         sci.electronics       1.00      1.00      1.00        84\n",
      "                 sci.med       1.00      1.00      1.00        83\n",
      "               sci.space       1.00      1.00      1.00        79\n",
      "  soc.religion.christian       0.99      1.00      0.99        77\n",
      "      talk.politics.guns       0.99      1.00      0.99        72\n",
      "   talk.politics.mideast       0.99      1.00      0.99        80\n",
      "      talk.politics.misc       0.99      0.98      0.98        82\n",
      "      talk.religion.misc       0.97      0.88      0.92        76\n",
      "\n",
      "                accuracy                           0.99      1600\n",
      "               macro avg       0.99      0.99      0.99      1600\n",
      "            weighted avg       0.99      0.99      0.99      1600\n",
      "\n",
      "Classification Report for test:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.54      0.83      0.65        18\n",
      "           comp.graphics       0.76      0.89      0.82        18\n",
      " comp.os.ms-windows.misc       0.95      0.91      0.93        22\n",
      "comp.sys.ibm.pc.hardware       0.84      0.84      0.84        25\n",
      "   comp.sys.mac.hardware       0.80      0.95      0.87        21\n",
      "          comp.windows.x       1.00      0.88      0.94        25\n",
      "            misc.forsale       1.00      0.72      0.84        18\n",
      "               rec.autos       0.94      0.94      0.94        18\n",
      "         rec.motorcycles       0.94      0.94      0.94        16\n",
      "      rec.sport.baseball       0.84      0.89      0.86        18\n",
      "        rec.sport.hockey       0.88      1.00      0.94        15\n",
      "               sci.crypt       0.83      1.00      0.90        19\n",
      "         sci.electronics       0.76      0.81      0.79        16\n",
      "                 sci.med       0.88      0.88      0.88        17\n",
      "               sci.space       1.00      0.90      0.95        21\n",
      "  soc.religion.christian       0.85      0.96      0.90        23\n",
      "      talk.politics.guns       0.92      0.79      0.85        28\n",
      "   talk.politics.mideast       0.95      0.95      0.95        20\n",
      "      talk.politics.misc       0.73      0.89      0.80        18\n",
      "      talk.religion.misc       0.57      0.17      0.26        24\n",
      "\n",
      "                accuracy                           0.85       400\n",
      "               macro avg       0.85      0.86      0.84       400\n",
      "            weighted avg       0.85      0.85      0.84       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report for train:\\n\", classification_report(y_train, model.predict(x_train)))\n",
    "print(\"Classification Report for test:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2= GaussianNB()\n",
    "model2.fit(x_train.toarray(), y_train)\n",
    "y_pred_G = model2.predict(x_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for train:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       1.00      1.00      1.00        82\n",
      "           comp.graphics       1.00      1.00      1.00        82\n",
      " comp.os.ms-windows.misc       1.00      1.00      1.00        78\n",
      "comp.sys.ibm.pc.hardware       1.00      1.00      1.00        75\n",
      "   comp.sys.mac.hardware       1.00      1.00      1.00        79\n",
      "          comp.windows.x       1.00      1.00      1.00        75\n",
      "            misc.forsale       1.00      1.00      1.00        82\n",
      "               rec.autos       1.00      1.00      1.00        82\n",
      "         rec.motorcycles       1.00      1.00      1.00        84\n",
      "      rec.sport.baseball       1.00      1.00      1.00        82\n",
      "        rec.sport.hockey       1.00      1.00      1.00        85\n",
      "               sci.crypt       1.00      1.00      1.00        81\n",
      "         sci.electronics       1.00      1.00      1.00        84\n",
      "                 sci.med       1.00      1.00      1.00        83\n",
      "               sci.space       1.00      1.00      1.00        79\n",
      "  soc.religion.christian       1.00      1.00      1.00        77\n",
      "      talk.politics.guns       1.00      1.00      1.00        72\n",
      "   talk.politics.mideast       1.00      1.00      1.00        80\n",
      "      talk.politics.misc       1.00      1.00      1.00        82\n",
      "      talk.religion.misc       1.00      1.00      1.00        76\n",
      "\n",
      "                accuracy                           1.00      1600\n",
      "               macro avg       1.00      1.00      1.00      1600\n",
      "            weighted avg       1.00      1.00      1.00      1600\n",
      "\n",
      "Classification Report for test:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.53      0.56      0.54        18\n",
      "           comp.graphics       0.34      0.72      0.46        18\n",
      " comp.os.ms-windows.misc       0.71      0.55      0.62        22\n",
      "comp.sys.ibm.pc.hardware       0.71      0.48      0.57        25\n",
      "   comp.sys.mac.hardware       0.67      0.48      0.56        21\n",
      "          comp.windows.x       0.65      0.60      0.62        25\n",
      "            misc.forsale       0.60      0.33      0.43        18\n",
      "               rec.autos       0.64      0.50      0.56        18\n",
      "         rec.motorcycles       0.79      0.69      0.73        16\n",
      "      rec.sport.baseball       0.88      0.83      0.86        18\n",
      "        rec.sport.hockey       0.82      0.93      0.88        15\n",
      "               sci.crypt       0.74      0.89      0.81        19\n",
      "         sci.electronics       0.50      0.38      0.43        16\n",
      "                 sci.med       0.75      0.71      0.73        17\n",
      "               sci.space       0.78      0.86      0.82        21\n",
      "  soc.religion.christian       0.86      0.78      0.82        23\n",
      "      talk.politics.guns       0.77      0.61      0.68        28\n",
      "   talk.politics.mideast       0.62      0.90      0.73        20\n",
      "      talk.politics.misc       0.40      0.78      0.53        18\n",
      "      talk.religion.misc       0.56      0.42      0.48        24\n",
      "\n",
      "                accuracy                           0.64       400\n",
      "               macro avg       0.67      0.65      0.64       400\n",
      "            weighted avg       0.67      0.64      0.64       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report for train:\\n\", classification_report(y_train, model2.predict(x_train.toarray())))\n",
    "print(\"Classification Report for test:\\n\", classification_report(y_test, y_pred_G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(cleaned_Data):\n",
    "    blob = TextBlob(cleaned_Data)\n",
    "    sentiment = blob.sentiment.polarity\n",
    "    if sentiment > 0:\n",
    "        return 'positive'\n",
    "    elif sentiment < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Labels</th>\n",
       "      <th>cleaned_Data</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>path cantaloupesrvcscmuedumagnesiumclubcccmued...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Newsgroups: alt.atheism\\nPath: cantaloupe.srv....</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>newsgroups altatheism path cantaloupesrvcscmue...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>path cantaloupesrvcscmuedudasnewsharvardedunoc...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>path cantaloupesrvcscmuedumagnesiumclubcccmued...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>xref cantaloupesrvcscmuedu altatheism talkreli...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu talk.abortion:...</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>xref cantaloupesrvcscmuedu talkabortion altath...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu talk.religion....</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>xref cantaloupesrvcscmuedu talkreligionmisc ta...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu talk.origins:4...</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>xref cantaloupesrvcscmuedu talkorigins talkrel...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu talk.religion....</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>xref cantaloupesrvcscmuedu talkreligionmisc al...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.skeptic:43...</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>xref cantaloupesrvcscmuedu sciskeptic talkpoli...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Data              Labels  \\\n",
       "0     Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...         alt.atheism   \n",
       "1     Newsgroups: alt.atheism\\nPath: cantaloupe.srv....         alt.atheism   \n",
       "2     Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...         alt.atheism   \n",
       "3     Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...         alt.atheism   \n",
       "4     Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53...         alt.atheism   \n",
       "...                                                 ...                 ...   \n",
       "1995  Xref: cantaloupe.srv.cs.cmu.edu talk.abortion:...  talk.religion.misc   \n",
       "1996  Xref: cantaloupe.srv.cs.cmu.edu talk.religion....  talk.religion.misc   \n",
       "1997  Xref: cantaloupe.srv.cs.cmu.edu talk.origins:4...  talk.religion.misc   \n",
       "1998  Xref: cantaloupe.srv.cs.cmu.edu talk.religion....  talk.religion.misc   \n",
       "1999  Xref: cantaloupe.srv.cs.cmu.edu sci.skeptic:43...  talk.religion.misc   \n",
       "\n",
       "                                           cleaned_Data Sentiment  \n",
       "0     path cantaloupesrvcscmuedumagnesiumclubcccmued...  positive  \n",
       "1     newsgroups altatheism path cantaloupesrvcscmue...  negative  \n",
       "2     path cantaloupesrvcscmuedudasnewsharvardedunoc...  positive  \n",
       "3     path cantaloupesrvcscmuedumagnesiumclubcccmued...  positive  \n",
       "4     xref cantaloupesrvcscmuedu altatheism talkreli...  positive  \n",
       "...                                                 ...       ...  \n",
       "1995  xref cantaloupesrvcscmuedu talkabortion altath...  positive  \n",
       "1996  xref cantaloupesrvcscmuedu talkreligionmisc ta...  positive  \n",
       "1997  xref cantaloupesrvcscmuedu talkorigins talkrel...  positive  \n",
       "1998  xref cantaloupesrvcscmuedu talkreligionmisc al...  positive  \n",
       "1999  xref cantaloupesrvcscmuedu sciskeptic talkpoli...  positive  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog['Sentiment'] = blog['cleaned_Data'].apply(get_sentiment)\n",
    "blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "positive    1453\n",
       "negative     544\n",
       "neutral        3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_counts = blog['Sentiment'].value_counts()\n",
    "sentiment_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette='viridis')\n",
    "plt.title('Sentiment Distribution in Blog Posts')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Blogs: 2000\n",
      "Positive Blogs: 1453 (72.65%)\n",
      "Negative Blogs: 544 (27.20%)\n",
      "Neutral Blogs: 3 (0.15%)\n"
     ]
    }
   ],
   "source": [
    "total_posts = len(blog)\n",
    "positive_posts = sentiment_counts.get('positive', 0)\n",
    "negative_posts = sentiment_counts.get('negative', 0)\n",
    "neutral_posts = sentiment_counts.get('neutral', 0)\n",
    "\n",
    "print(f\"Total Blogs: {total_posts}\")\n",
    "print(f\"Positive Blogs: {positive_posts} ({positive_posts/total_posts:.2%})\")\n",
    "print(f\"Negative Blogs: {negative_posts} ({negative_posts/total_posts:.2%})\")\n",
    "print(f\"Neutral Blogs: {neutral_posts} ({neutral_posts/total_posts:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Training Set Performance:***\n",
    "1. **Overall Accuracy:** 99%\n",
    "2. **Precision:** Average of 99%, which means that most of the predicted labels are correct.\n",
    "3. **Recall:** Average of 99%, indicating that the model identifies almost all instances of each class.\n",
    "4. **F1-Score:** Average of 99%, a balance of precision and recall, showing overall high performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs exceptionally well on the training set, with near-perfect scores across all metrics. This suggests that the model is well-trained and fits the training data very effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Test Set Performance:***\n",
    "1. **Overall Accuracy:** 85%\n",
    "2. **Precision:** Average of 85%, which is lower than the training set but still indicates good performance in terms of correctly identifying positive instances of each class.\n",
    "3. **Recall:** Average of 86%, slightly higher than precision, showing good coverage of each class.\n",
    "4. **F1-Score:** Average of 84%, reflecting a balance between precision and recall, though lower than on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Drop in Performance:** There is a significant drop in performance from the training set to the test set. The accuracy, precision, recall, and F1-score on the test set are notably lower. This indicates that the model struggles to generalize to unseen data, which is a common issue known as overfitting.\n",
    "2. **Consistency:** While precision, recall, and F1-scores are still good, the drop shows that the model's predictions are less reliable on new data compared to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment analysis reveals that the blog posts primarily express positive sentiments, with a notable portion also expressing negative sentiments. The very low neutral sentiment suggests that the content is highly opinionated or emotionally charged. By reflecting on these results, you can refine your content strategy to enhance positive engagement, address negative feedback, and consider incorporating more neutral content if it aligns with your goals."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "z6hN1kdKbO89",
    "k7LUvQsqbO89"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
